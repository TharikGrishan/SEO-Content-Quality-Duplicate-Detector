{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d33ffe3c-4b5c-4f1e-befa-477a12eabc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking/downloading NLTK 'punkt' and 'punkt_tab' resources...\n",
      "NLTK downloads complete.\n",
      "\n",
      "Loaded dataset with 81 rows.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "from joblib import dump, load\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import textstat \n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "\n",
    "try:\n",
    "    print(\"Checking/downloading NLTK 'punkt' and 'punkt_tab' resources...\")\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('punkt_tab', quiet=True) \n",
    "    print(\"NLTK downloads complete.\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Error during NLTK download. Feature engineering may fail. ({e})\")\n",
    "\n",
    "# --- Configuration and Paths ---\n",
    "DATA_DIR = 'data'\n",
    "MODELS_DIR = 'models'\n",
    "\n",
    "INPUT_DATA_PATH = os.path.join(os.pardir, DATA_DIR, 'data.csv')\n",
    "EXTRACTED_CONTENT_PATH = os.path.join(os.pardir, DATA_DIR, 'extracted_content.csv')\n",
    "FEATURES_PATH = os.path.join(os.pardir, DATA_DIR, 'features.csv')\n",
    "DUPLICATES_PATH = os.path.join(os.pardir, DATA_DIR, 'duplicates.csv')\n",
    "MODEL_PATH = os.path.join(os.pardir, MODELS_DIR, 'quality_model.pkl')\n",
    "VECTORIZER_PATH = os.path.join(os.pardir, MODELS_DIR, 'tfidf_vectorizer.pkl')\n",
    "\n",
    "SIMILARITY_THRESHOLD = 0.80\n",
    "THIN_CONTENT_WORD_COUNT = 500\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_csv(INPUT_DATA_PATH)\n",
    "    print(f\"\\nLoaded dataset with {len(df)} rows.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"\\nError: data.csv not found at {os.path.abspath(INPUT_DATA_PATH)}\")\n",
    "    print(\"Please ensure your 'data.csv' is inside the 'data' folder, and the 'data' folder is next to the 'notebooks' folder.\")\n",
    "    raise \n",
    "\n",
    "# --- HTML Parsing Function (Utility) ---\n",
    "def parse_html_content(html_content):\n",
    "    \"\"\"Parses HTML to extract title, body text, and word count.\"\"\"\n",
    "    if pd.isna(html_content) or not html_content:\n",
    "        return 'No Title', '', 0\n",
    "    \n",
    "    try:\n",
    "        soup = BeautifulSoup(str(html_content), 'html.parser')\n",
    "        \n",
    "        title = soup.title.string.strip() if soup.title and soup.title.string else 'No Title'\n",
    "        \n",
    "        main_content_div = soup.find('main') or soup.find('article') or soup.find('body')\n",
    "        \n",
    "        if main_content_div:\n",
    "            body_text = main_content_div.get_text(separator=' ', strip=True)\n",
    "        else:\n",
    "            body_text = soup.body.get_text(separator=' ', strip=True) if soup.body else ''\n",
    "\n",
    "        clean_text = re.sub(r'\\s+', ' ', body_text).strip()\n",
    "        word_count = len(clean_text.split())\n",
    "\n",
    "        return title, clean_text, word_count\n",
    "        \n",
    "    except Exception as e:\n",
    "        return 'Parsing Error', '', 0 \n",
    "\n",
    "# --- Text Feature Function (Utility) ---\n",
    "def calculate_text_features(text):\n",
    "    \"\"\"Calculates sentence count and Flesch Reading Ease score.\"\"\"\n",
    "    if not text:\n",
    "        return (0, 0.0)\n",
    "        \n",
    "    clean_text = str(text).lower()\n",
    "    \n",
    "    sentence_count = len(sent_tokenize(clean_text))\n",
    "    \n",
    "    try:\n",
    "        flesch_score = textstat.flesch_reading_ease(clean_text)\n",
    "    except:\n",
    "        flesch_score = 0.0 \n",
    "        \n",
    "    return (sentence_count, flesch_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4780d074-63c0-4f38-8e3f-8d84a4aff0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 2: HTML Parsing and Content Extraction (15%) ---\n",
      "Parsed 81 pages. Saved to ..\\data\\extracted_content.csv\n",
      "                                                 url  \\\n",
      "0     https://www.cm-alliance.com/cybersecurity-blog   \n",
      "1    https://www.varonis.com/blog/cybersecurity-tips   \n",
      "2  https://www.cisecurity.org/insights/blog/11-cy...   \n",
      "3  https://www.cisa.gov/topics/cybersecurity-best...   \n",
      "4  https://www.qnbtrust.bank/Resources/Learning-C...   \n",
      "\n",
      "                                               title  \\\n",
      "0                                Cyber Security Blog   \n",
      "1  Top 10 Cybersecurity Awareness Tips: How to St...   \n",
      "2  11 Cyber Defense Tips to Stay Secure at Work a...   \n",
      "3  Cybersecurity Best Practices | Cybersecurity a...   \n",
      "4                                           No Title   \n",
      "\n",
      "                                           body_text  word_count  \n",
      "0  Back Training NCSC Assured Cyber Incident Plan...        2605  \n",
      "1  Blog Privacy & Compliance Top 10 Cybersecurity...        1747  \n",
      "2  Home Insights Blog Posts 11 Cyber Defense Tips...        1058  \n",
      "3  Cybersecurity Best Practices CISA provides inf...         826  \n",
      "4                                                              0  \n"
     ]
    }
   ],
   "source": [
    "print(\"---HTML Parsing and Content Extraction---\")\n",
    "\n",
    "# Apply the parsing function\n",
    "df[['title', 'body_text', 'word_count']] = df['html_content'].apply(\n",
    "    lambda x: pd.Series(parse_html_content(x))\n",
    ")\n",
    "\n",
    "extracted_df = df[['url', 'title', 'body_text', 'word_count']].copy()\n",
    "extracted_df.to_csv(EXTRACTED_CONTENT_PATH, index=False)\n",
    "print(f\"Parsed {len(extracted_df)} pages. Saved to {EXTRACTED_CONTENT_PATH}\")\n",
    "print(extracted_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd610e35-d315-4553-bf39-4d7b791752e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to download 'punkt_tab' resource for sentence tokenization...\n",
      "Download complete. Proceeding with feature calculation.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "try:\n",
    "    print(\"Attempting to download 'punkt_tab' resource for sentence tokenization...\")\n",
    "    nltk.download('punkt_tab', quiet=True)\n",
    "    print(\"Download complete. Proceeding with feature calculation.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during NLTK download: {e}\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90b3ab41-0b26-4c77-97e8-f48c3f5808a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 3: Feature Engineering (25%) ---\n",
      "Features computed. Saved to ..\\data\\features.csv and vectorizer saved to ..\\models\\tfidf_vectorizer.pkl\n",
      "                                                 url  word_count  \\\n",
      "0     https://www.cm-alliance.com/cybersecurity-blog        2605   \n",
      "1    https://www.varonis.com/blog/cybersecurity-tips        1747   \n",
      "2  https://www.cisecurity.org/insights/blog/11-cy...        1058   \n",
      "3  https://www.cisa.gov/topics/cybersecurity-best...         826   \n",
      "4  https://www.qnbtrust.bank/Resources/Learning-C...           0   \n",
      "\n",
      "   sentence_count  flesch_reading_ease  \\\n",
      "0              66            28.741548   \n",
      "1              94            40.871699   \n",
      "2              62            53.262918   \n",
      "3              27            -2.538002   \n",
      "4               0             0.000000   \n",
      "\n",
      "                                    top_keywords  \\\n",
      "0     cyber|2025|cybersecurity|tabletop|incident   \n",
      "1         access|data|security|app|cybersecurity   \n",
      "2       password|authentication|don|cyber|secure   \n",
      "3  cisa|cybersecurity|cyber|practices|resilience   \n",
      "4                                                  \n",
      "\n",
      "                                           embedding  \n",
      "0  [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...  \n",
      "1  [0.0,0.019864361267619025,0.0,0.0,0.0,0.0,0.0,...  \n",
      "2  [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...  \n",
      "3  [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.020760403184743...  \n",
      "4  [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0...  \n"
     ]
    }
   ],
   "source": [
    "print(\"\\n---Feature Engineering---\")\n",
    "\n",
    "all_features_calculated = [\n",
    "    calculate_text_features(text) for text in extracted_df['body_text']\n",
    "]\n",
    "\n",
    "# Create a temporary DataFrame directly from this list of tuples.\n",
    "feature_df = pd.DataFrame(\n",
    "    all_features_calculated, \n",
    "    columns=['sentence_count', 'flesch_reading_ease'],\n",
    "    index=extracted_df.index \n",
    ")\n",
    "\n",
    "# Assign the columns from the new DataFrame\n",
    "extracted_df[['sentence_count', 'flesch_reading_ease']] = feature_df\n",
    "\n",
    "# TF-IDF and Embedding Generation\n",
    "content_for_tfidf = extracted_df[extracted_df['word_count'] > 0]['body_text']\n",
    "\n",
    "# Fit the vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=2000)\n",
    "tfidf_matrix = vectorizer.fit_transform(content_for_tfidf)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Keyword Extraction\n",
    "def get_top_keywords(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    tfidf_vector = vectorizer.transform([text])\n",
    "    \n",
    "    if tfidf_vector.shape[1] == 0:\n",
    "        return \"\" \n",
    "\n",
    "    top_n = 5\n",
    "    sorted_indices = tfidf_vector.sum(axis=0).A1.argsort()[-top_n:][::-1]\n",
    "    \n",
    "    keywords = [feature_names[i] for i in sorted_indices if tfidf_vector[0, i] > 0]\n",
    "    return '|'.join(keywords) \n",
    "\n",
    "extracted_df['top_keywords'] = extracted_df['body_text'].apply(get_top_keywords)\n",
    "\n",
    "# Create the 'embedding' column \n",
    "full_tfidf_matrix = vectorizer.transform(extracted_df['body_text'])\n",
    "\n",
    "extracted_df['embedding'] = [\n",
    "    str(list(vector)).replace(' ', '') for vector in full_tfidf_matrix.toarray()\n",
    "]\n",
    "\n",
    "# Save features and vectorizer\n",
    "features_df = extracted_df[['url', 'word_count', 'sentence_count', 'flesch_reading_ease', 'top_keywords', 'embedding']].copy()\n",
    "features_df.to_csv(FEATURES_PATH, index=False)\n",
    "dump(vectorizer, VECTORIZER_PATH) \n",
    "print(f\"Features computed. Saved to {FEATURES_PATH} and vectorizer saved to {VECTORIZER_PATH}\")\n",
    "print(features_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9107fb62-1e11-4f63-a8e6-28a95512b427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 4: Duplicate Detection (20%) ---\n",
      "Total pages analyzed: 81\n",
      "Duplicate pairs found: 7\n",
      "Thin content pages: 24 (29.6%)\n",
      "Saved duplicate pairs to ..\\data\\duplicates.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n---Duplicate Detection---\")\n",
    "\n",
    "# Compute pairwise cosine similarity\n",
    "cosine_sim_matrix = cosine_similarity(tfidf_matrix)\n",
    "valid_indices = extracted_df[extracted_df['word_count'] > 0].index\n",
    "\n",
    "duplicate_pairs = []\n",
    "n_valid_pages = tfidf_matrix.shape[0]\n",
    "\n",
    "for i in range(n_valid_pages):\n",
    "    for j in range(i + 1, n_valid_pages):\n",
    "        similarity = cosine_sim_matrix[i, j]\n",
    "        if similarity >= SIMILARITY_THRESHOLD: \n",
    "            url1 = extracted_df.loc[valid_indices[i], 'url']\n",
    "            url2 = extracted_df.loc[valid_indices[j], 'url']\n",
    "            duplicate_pairs.append({\n",
    "                'url1': url1, \n",
    "                'url2': url2, \n",
    "                'similarity': round(similarity, 4)\n",
    "            })\n",
    "\n",
    "# Save duplicate pairs\n",
    "duplicates_df = pd.DataFrame(duplicate_pairs)\n",
    "duplicates_df.to_csv(DUPLICATES_PATH, index=False)\n",
    "\n",
    "# Thin Content Detection\n",
    "extracted_df['is_thin'] = (extracted_df['word_count'] < THIN_CONTENT_WORD_COUNT)\n",
    "\n",
    "# Report basic statistics\n",
    "total_pages = len(extracted_df)\n",
    "num_duplicates = len(duplicates_df)\n",
    "num_thin = extracted_df['is_thin'].sum()\n",
    "percent_thin = (num_thin / total_pages) * 100 if total_pages > 0 else 0\n",
    "\n",
    "print(f\"Total pages analyzed: {total_pages}\")\n",
    "print(f\"Duplicate pairs found: {num_duplicates}\")\n",
    "print(f\"Thin content pages: {num_thin} ({percent_thin:.1f}%)\")\n",
    "print(f\"Saved duplicate pairs to {DUPLICATES_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07b96273-3a89-4964-9bcf-b32bb71cff41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 5: Content Quality Scoring (25%) ---\n",
      "Overall Accuracy: 0.7143\n",
      "\n",
      "Classification Report (F1-score):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.00      0.00      0.00         2\n",
      "         Low       0.83      0.91      0.87        11\n",
      "      Medium       0.62      0.62      0.62         8\n",
      "\n",
      "    accuracy                           0.71        21\n",
      "   macro avg       0.49      0.51      0.50        21\n",
      "weighted avg       0.67      0.71      0.69        21\n",
      "\n",
      "Saved quality model to ..\\models\\quality_model.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n---Content Quality Scoring---\")\n",
    "\n",
    "def create_synthetic_label(row):\n",
    "    \"\"\"Creates synthetic labels (High, Medium, Low) based on word count and readability.\"\"\"\n",
    "    word_count = row['word_count']\n",
    "    readability = row['flesch_reading_ease']\n",
    "    \n",
    "    if word_count > 1500 and 50 <= readability <= 70:\n",
    "        return 'High'\n",
    "    elif word_count < 500 or readability < 30:\n",
    "        return 'Low'\n",
    "    else:\n",
    "        return 'Medium'\n",
    "\n",
    "extracted_df['quality_label'] = extracted_df.apply(create_synthetic_label, axis=1)\n",
    "\n",
    "# Prepare Features (X) and Target (y) for pages with content\n",
    "model_data = extracted_df[extracted_df['word_count'] > 0].copy()\n",
    "X_core = model_data[['word_count', 'sentence_count', 'flesch_reading_ease']]\n",
    "y = model_data['quality_label']\n",
    "\n",
    "# Re-align TF-IDF matrix for training data only\n",
    "tfidf_train_data = vectorizer.transform(model_data['body_text'])\n",
    "\n",
    "# Combine core features and TF-IDF features\n",
    "X = pd.concat([X_core.reset_index(drop=True), \n",
    "               pd.DataFrame(tfidf_train_data.toarray()).reset_index(drop=True)], axis=1)\n",
    "X.columns = X.columns.astype(str)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Classification Model (Logistic Regression)\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model.fit(X_train, y_train) \n",
    "\n",
    "# Predict and Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Overall Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report (F1-score):\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save the trained model\n",
    "dump(model, MODEL_PATH)\n",
    "print(f\"Saved quality model to {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5def9cf9-fbba-4930-a3a9-9cfcb9aeaa0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing Real-Time Analysis Demo ---\n",
      "Analyzing URL: https://en.wikipedia.org/wiki/Digital_marketing\n",
      "{\n",
      "  \"url\": \"https://en.wikipedia.org/wiki/Digital_marketing\",\n",
      "  \"word_count\": 11368,\n",
      "  \"readability\": 35.7,\n",
      "  \"quality_label\": \"Medium\",\n",
      "  \"is_thin\": false,\n",
      "  \"similar_to\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import warnings\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "\n",
    "warnings.simplefilter('ignore', InsecureRequestWarning)\n",
    "\n",
    "\n",
    "# --- Live Scraping Utility ---\n",
    "def realtime_scrape_url(url):\n",
    "    \"\"\"Scrapes a single URL with error handling and rate limiting.\"\"\"\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (compatible; SEOContentAnalyzer/1.0)'} \n",
    "        time.sleep(1.5) \n",
    "        response = requests.get(url, headers=headers, timeout=10, verify=False)\n",
    "        response.raise_for_status() \n",
    "        return response.text\n",
    "    except requests.exceptions.RequestException as e: \n",
    "        print(f\"Scraping error for {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Main Analysis Function ---\n",
    "def analyze_url(url, existing_df=extracted_df):\n",
    "    try:\n",
    "        realtime_model = load(MODEL_PATH)\n",
    "        realtime_vectorizer = load(VECTORIZER_PATH)\n",
    "    except FileNotFoundError:\n",
    "        return {\"error\": \"Model or vectorizer not found. Run the full pipeline first.\"}\n",
    "\n",
    "    # Scraping \n",
    "    html_content = realtime_scrape_url(url)\n",
    "    if html_content is None:\n",
    "        return {\"url\": url, \"error\": \"Scraping failed or received bad status code.\"}\n",
    "\n",
    "    # Parsing and Feature Extraction\n",
    "    title, body_text, word_count = parse_html_content(html_content)\n",
    "    if not body_text:\n",
    "        return {\"url\": url, \"error\": \"Parsing failed or content is empty.\"}\n",
    "        \n",
    "    # Recalculate features (returns a tuple)\n",
    "    sentence_count, readability = calculate_text_features(body_text)\n",
    "\n",
    "    # Model Prediction Preparation\n",
    "    X_core_features = pd.DataFrame({\n",
    "        'word_count': [word_count],\n",
    "        'sentence_count': [sentence_count],\n",
    "        'flesch_reading_ease': [readability]\n",
    "    })\n",
    "    \n",
    "    # TF-IDF vector\n",
    "    new_tfidf_vector = realtime_vectorizer.transform([body_text])\n",
    "    \n",
    "    # Combine features (must match training feature count and column string type)\n",
    "    X_predict = pd.concat([X_core_features.reset_index(drop=True), \n",
    "                           pd.DataFrame(new_tfidf_vector.toarray()).reset_index(drop=True)], axis=1)\n",
    "    \n",
    "    # Ensure column names are strings for prediction\n",
    "    X_predict.columns = X_predict.columns.astype(str)\n",
    "\n",
    "    # Quality Score\n",
    "    quality_label = realtime_model.predict(X_predict)[0]\n",
    "    \n",
    "    # Duplicate Check\n",
    "    corpus_content_df = existing_df[existing_df['body_text'].str.len() > 0].copy()\n",
    "    corpus_tfidf = realtime_vectorizer.transform(corpus_content_df['body_text'])\n",
    "    corpus_urls = corpus_content_df['url']\n",
    "    \n",
    "    # Calculate similarity \n",
    "    new_sims = cosine_similarity(new_tfidf_vector, corpus_tfidf)[0]\n",
    "    \n",
    "    similar_to = []\n",
    "    for i, sim in enumerate(new_sims):\n",
    "        if sim >= SIMILARITY_THRESHOLD and sim < 0.999: \n",
    "            similar_to.append({\n",
    "                \"url\": corpus_urls.iloc[i], \n",
    "                \"similarity\": round(sim, 4)\n",
    "            })\n",
    "\n",
    "    # Final Output\n",
    "    return {\n",
    "        \"url\": url,\n",
    "        \"word_count\": word_count,\n",
    "        \"readability\": round(readability, 1),\n",
    "        \"quality_label\": quality_label,\n",
    "        \"is_thin\": bool(word_count < THIN_CONTENT_WORD_COUNT),\n",
    "        \"similar_to\": similar_to\n",
    "    }\n",
    "\n",
    "print(\"\\n--- Testing Real-Time Analysis Demo ---\")\n",
    "test_url = \"https://en.wikipedia.org/wiki/Digital_marketing\"\n",
    "print(f\"Analyzing URL: {test_url}\")\n",
    "result = analyze_url(test_url)\n",
    "print(json.dumps(result, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
